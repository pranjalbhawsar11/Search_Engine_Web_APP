# -*- coding: utf-8 -*-
"""Group_20.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14w9Elu8LvI6G1LccXB1LQJ9cXOYQULGQ
"""

from google.colab import drive
drive.mount('/content/drive')

from xml.dom import minidom

# doc = minidom.parse("/content/drive/MyDrive/test_set/test.xml")
doc = minidom.parse("/content/drive/MyDrive/enwiki-20221201-pages-articles-multistream1.xml-p1p41242")

pages = doc.getElementsByTagName("page")

import csv
fieldnames = ['Title_id', 'Title','Body']
    

with open('/content/drive/MyDrive/titles.csv', 'w') as csvfile:
    writer = csv.DictWriter(csvfile,fieldnames=fieldnames)
    writer.writeheader()
    for page in pages:
        title_id=page.getElementsByTagName("id")[0]
        title = page.getElementsByTagName("title")[0]
        text = page.getElementsByTagName("text")[0]
        writer.writerow({'Title_id':title_id.firstChild.data,'Title':title.firstChild.data,'Body':text.firstChild.data})

import pandas as pd

data = pd.read_csv("/content/drive/MyDrive/titles.csv")
dft = data[["Title_id", "Body"]]
dfq= data[["Title_id","Body"]]

x = data.iloc[:1000, [0]].values
y = data.iloc[:, [1]].values
z = data.iloc[:1000, [2]].values

"""#### TFIDF and BM25"""

import nltk
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer

# Assuming doc_list is a list of documents to be indexed
nltk.download('stopwords')
stop_words = set(stopwords.words('english'))
stemmer = PorterStemmer()

def preprocess(text):
    # Remove punctuation and convert to lowercase
    text = ''.join([c for c in text if c.isalpha() or c.isspace()]).lower()
    # Remove stop words
    tokens = text.split()
    tokens = [token for token in tokens if token not in stop_words]
    # Stem the remaining words
    tokens = [stemmer.stem(token) for token in tokens]
    return ' '.join(tokens)

# Preprocess the documents
preprocessed_docs = [preprocess(doc) for doc in z]

from sklearn.feature_extraction.text import TfidfVectorizer

documents = (' '.join(doc) for doc in z)
tfidf = TfidfVectorizer(min_df=5)
tfidf_scores = tfidf.fit_transform(documents)

import numpy as np

tfidf_scores_arr = np.array(tfidf_scores)

tfidf_scores_arr = np.nan_to_num(tfidf_scores_arr)
tfidf_scores_norm = (tfidf_scores_arr - tfidf_scores_arr.min()) / (tfidf_scores_arr.max() - tfidf_scores_arr.min())

from gensim.summarization.bm25 import BM25

tokenized_docs = [doc.split() for doc in z.flatten()]
tokenized_docs_flat = [token for doc in tokenized_docs for token in doc]

bm25_model = BM25(tokenized_docs)

import numpy as np

idf = bm25_model.idf

avg_idf = np.mean(list(idf.values()))
bm25_scores = bm25_model.get_scores(tokenized_docs_flat, avg_idf)

bm25_scores_arr = np.array(bm25_scores)
bm25_scores_norm = (bm25_scores_arr - bm25_scores_arr.min()) / (bm25_scores_arr.max() - bm25_scores_arr.min())

"""#### Extracting Substrings for HITS and PageRank"""

import re
def extract_substrings(s):
    pattern = r'\[\[(.*?)\]\]'
    substrings = re.findall(pattern, s)
    substrings_with_pipe = [ss.split('|') for ss in substrings]
    flattened_substrings = [s.strip() for sublist in substrings_with_pipe for s in sublist]
    return flattened_substrings


title_to_id={}
id_to_title={}

for i, (id, title) in enumerate(zip(x,y)):
  title_to_id[str(title[0])] = id[0]
  id_to_title[id[0]]=str(title[0])

title_map = {}
edges=[]

for i, (id, text) in enumerate(zip(x,z)):
  a=[]
  s=extract_substrings(str(text))
  for link in s:
    if link in title_to_id:
      a.append(title_to_id[link])
      edges.append(tuple((id[0],title_to_id[link])))
      # edges.append(tuple((id_to_title[id[0]],title_to_id[link])))
  title_map[id[0]]=a

import networkx as nx
G = nx.DiGraph()
G.clear()
G.add_edges_from(edges)

pr = nx.pagerank(G, 0.4)
pr_sorted = sorted(pr.items(), key=lambda x: x[1], reverse=True)
print(pr_sorted)

# Top Five Documents ar

hubs, authorities = nx.hits(G, max_iter = 50, normalized = True)
print("Hub Scores: ", hubs)
print("Authority Scores: ", authorities)
hubs_authorities = []
for node in hubs:
    hubs_authorities.append(hubs[node] + authorities[node])
hubs_authorities =  sorted(hubs_authorities)
print(hubs_authorities)

pagerank_norm = {k: v / max(pr.values()) for k, v in pr.items()}
hubs_norm = {k: v / max(hubs.values()) for k, v in hubs.items()}
authorities_norm = {k: v / max(authorities.values()) for k, v in authorities.items()}

pagerank_norm_arr = np.array(list(pagerank_norm.values())).reshape(-1, 1)
hubs_authorities_norm_arr = np.array(list(hubs_norm.values())+ list(authorities_norm.values())).reshape(-1, 1)

tfidf_scores_arr = tfidf_scores_norm.toarray()
bm25_scores_arr = np.array(bm25_scores_norm)
combined_score = []
for i in range(5):
  t = 0.25 * tfidf_scores_arr[i] + 0.25 * bm25_scores_arr[i] + 0.25 * pagerank_norm_arr[i] + 0.25 * hubs_authorities_norm_arr[i]
  combined_score.append(t)

for i in range(5):
  print(combined_score[i])